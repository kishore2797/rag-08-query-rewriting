# ğŸ”„ RAG Tutorial 08 â€” Query Rewriting & Expansion

<p align="center">
  <a href="https://github.com/kishore2797/mastering-rag"><img src="https://img.shields.io/badge/Series-Mastering_RAG-blue?style=for-the-badge" /></a>
  <img src="https://img.shields.io/badge/Part-8_of_16-green?style=for-the-badge" />
  <img src="https://img.shields.io/badge/Difficulty-Intermediate-yellow?style=for-the-badge" />
</p>

> **Part of the [Mastering RAG](https://github.com/kishore2797/mastering-rag) tutorial series**  
> Previous: [07 â€” Parent-Document Retrieval](https://github.com/kishore2797/rag-07-parent-document-retrieval) | Next: [09 â€” Re-ranking Pipeline](https://github.com/kishore2797/rag-09-reranking-pipeline)

---

## ğŸŒ Real-World Scenario

> A user asks your customer support bot: "it's not working." That's it. No product name, no error message, no context. Standard vector search finds... everything. **Multi-query expansion** rewrites it into: ["product not functioning properly", "error when using the application", "troubleshooting common issues"]. **Query decomposition** breaks it into: "What product is the user referring to?", "What error are they experiencing?". Suddenly, retrieval gets 3x better â€” all without asking the user to clarify.

---

## ğŸ—ï¸ What You'll Build

A RAG system with three query optimization strategies: **HyDE** (Hypothetical Document Embedding), **multi-query expansion**, and **query decomposition**. Toggle each strategy on/off and see how retrieval results change.

```
"Why is my app slow?"
  â†“
HyDE:    Generate hypothetical answer â†’ embed that instead of the raw query
Multi-Q: Expand to 3 rephrased queries â†’ retrieve for each â†’ merge results
Decomp:  Break into sub-questions â†’ retrieve per sub-question â†’ combine
```

## ğŸ”‘ Key Concepts

- **HyDE**: bridges the vocabulary gap between short queries and long documents
- **Multi-query**: catches different phrasings of the same user intent
- **Decomposition**: handles complex, multi-part questions
- **Retrieval improvement**: measure recall before/after rewriting

## ğŸ›  Tech Stack

| Layer | Technology |
|-------|-----------|
| Backend | Python 3.11+ Â· FastAPI Â· ChromaDB Â· OpenAI/Gemini Â· LangChain |
| Frontend | React 19 Â· Vite Â· Tailwind CSS |

## ğŸš€ Quick Start

### Backend

```bash
cd backend
python3 -m venv .venv && source .venv/bin/activate
pip install -r requirements.txt
cp .env.example .env
uvicorn app.main:app --reload --port 8001
```

### Frontend

```bash
cd frontend
npm install
npm run dev
```

Open http://localhost:5173 â€” run queries with different rewriting strategies and compare.

## ğŸ“– What You'll Learn

1. Why raw user queries often perform poorly in vector search
2. How HyDE generates synthetic documents to improve retrieval
3. When multi-query expansion helps (ambiguous queries)
4. When decomposition helps (complex, multi-part questions)
5. How to measure the retrieval improvement from each strategy

## ğŸ“‹ Prerequisites

- Python 3.11+ and Node.js 18+
- A working RAG pipeline (concepts from [Tutorial 05](https://github.com/kishore2797/rag-05-basic-rag-pipeline))
- LLM API key (needed for HyDE and decomposition)

## âœï¸ Exercises

1. **Head-to-head**: Use 20 test queries. Run each with raw query, HyDE, multi-query, and decomposition. Score which retrieves the most relevant chunks. Create a leaderboard.
2. **Latency measurement**: HyDE and decomposition need an extra LLM call. Measure the latency overhead. Is the quality improvement worth the extra 1â€“2 seconds?
3. **Bad query stress test**: Try deliberately vague queries ("stuff about that thing"), typos ("mashine lerning"), and mixed-language queries. Which strategy recovers best?
4. **Combine strategies**: Try HyDE + multi-query together. Does combining strategies compound the improvement or add noise?
5. **Cost analysis**: Calculate the extra API cost per query for each strategy. At 10K queries/day, what's the monthly cost difference?

## âš ï¸ Common Mistakes

| Mistake | Why It Happens | How to Fix |
|---------|---------------|------------|
| HyDE generates a bad hypothetical answer | The LLM hallucinates, and you embed that hallucination | Keep HyDE answers short (2â€“3 sentences); use a low temperature |
| Multi-query returns too many duplicates | Same chunks match multiple rephrased queries | Deduplicate results by chunk ID before passing to the LLM |
| Decomposition creates irrelevant sub-questions | LLM over-decomposes a simple query into 5+ parts | Limit decomposition to 2â€“3 sub-questions; skip for short, clear queries |
| Rewriting makes queries worse | Original query was already precise; rewriting adds noise | Add a "confidence check" â€” only rewrite if the original query's retrieval score is below a threshold |

## ğŸ“š Further Reading

- [Precise Zero-Shot Dense Retrieval without Relevance Labels (HyDE)](https://arxiv.org/abs/2212.10496) â€” The original HyDE paper (Gao et al., 2022)
- [Query Expansion by Prompting Large Language Models](https://arxiv.org/abs/2305.03653) â€” Multi-query expansion research
- [Query2Doc](https://arxiv.org/abs/2303.07678) â€” Another approach to query expansion with LLMs
- [RAG Query Rewriting Best Practices](https://www.pinecone.io/learn/series/rag/rerankers/) â€” Pinecone's practical guide

## â¡ï¸ Next Steps

You've optimized the query. Now optimize the results â€” head to **[Tutorial 09 â€” Re-ranking Pipeline](https://github.com/kishore2797/rag-09-reranking-pipeline)** to add cross-encoder re-ranking after retrieval.

---

<p align="center">
  <sub>Part of <a href="https://github.com/kishore2797/mastering-rag">Mastering RAG â€” From Zero to Production</a></sub>
</p>
